---
title: "Exporting HTNPoRT Models"
format: html
editor: visual
---

## Summary

This guide provides a step-by-step approach for safely exporting survey-weighted GLM models from Statistics Canada while preserving the ability to make predictions and perform recalibration in new settings. The key challenge is removing all individual-level and identifiable data while retaining the statistical components necessary for valid inference.

**Key Steps:**

1.  Strip all individual-level data from the model object
2.  Preserve essential statistical components (coefficients, variance-covariance matrix)
3.  Extract and save transformation parameters (centering, scaling, spline knots)
4.  Document survey design metadata without individual identifiers
5.  Validate the export before transfer
6.  Implement recalibration methods in the new environment

## Understanding a svyglm object

A `svyglm` object inherits from both the `svyglm` and `glm` classes, containing standard GLM components plus survey-specific information:

**Standard GLM components:**

-   `coefficients`: Model parameter estimates
-   `residuals`: Individual-level residuals (MUST BE REMOVED)
-   `fitted.values`: Individual predictions (MUST BE REMOVED)
-   `model`: Original data frame (MUST BE REMOVED)
-   `family`: Distribution and link function (SAFE TO KEEP)
-   `formula`: Model specification (SAFE TO KEEP)
-   `qr`: QR decomposition of design matrix (REMOVE - not needed for prediction)

**Survey-specific components:**

-   `survey.design`: Contains survey weights, strata, clusters, and THE ORIGINAL DATA (MUST BE REMOVED)
-   `vcov`: Variance-covariance matrix using sandwich estimators (CRITICAL TO KEEP)
-   `df.residual`: Degrees of freedom accounting for survey design (KEEP)

The embedded survey design object poses the primary privacy risk as it contains all individual-level data.

## Step 1: Basic model stripping

Start with a basic function to remove individual-level data:

```{r}
strip_svyglm <- function(model) {
  # Remove all individual-level components
  model$y <- NULL
  model$model <- NULL
  model$residuals <- NULL
  model$fitted.values <- NULL
  model$effects <- NULL
  model$qr <- NULL
  model$linear.predictors <- NULL
  model$weights <- NULL
  model$prior.weights <- NULL
  model$data <- NULL
  
  # Remove the environment to prevent data access
  attr(model$terms, ".Environment") <- emptyenv()
  attr(model$formula, ".Environment") <- emptyenv()
  
  return(model)
}
```

## Step 2: Handling survey design information

The survey design object requires special care. We need to preserve the structure but remove individual data:

```{r}
clean_survey_design <- function(design) {
  # Create a summary that preserves structure without data
  design_summary <- list(
    nobs = nrow(design$variables),
    nstrata = length(unique(design$strata)),
    nclusters = length(unique(design$cluster)),
    has_weights = !is.null(design$prob),
    design_type = class(design)[1],
    call = design$call
  )
  
  # For finite population correction
  if (!is.null(design$fpc)) {
    design_summary$has_fpc <- TRUE
    design_summary$fpc_summary <- "FPC information available"
  }
  
  return(design_summary)
}
```

## Step 3: Preserving complex model features

### Centering and Scaling

When predictors are centered and scaled, you must preserve these transformations:

```{r}
extract_centering_info <- function(model, original_data_summary) {
  centering_info <- list()
  
  # Extract predictor names from model formula (excluding response)
  predictors <- all.vars(formula(model))[-1]
  
  for (var in predictors) {
    # Match all summary keys that start with var
    matching_keys <- grep(paste0("^", var), names(original_data_summary), value = TRUE)
    
    if (length(matching_keys) > 1) {
      centering_info[[var]] <- list()
      for (i in seq_along(matching_keys)) {
        mean_val <- original_data_summary[[matching_keys[i]]]$mean
        centering_info[[var]][[paste0("center", i)]] <- mean_val
      }
    } else if (var %in% names(original_data_summary)) {
      centering_info[[var]] <- list(center = original_data_summary[[var]]$mean)
    }
  }
  
  return(centering_info)
}
```

### Restricted cubic splines (RCS)

RCS terms require special handling because knot locations must be preserved:

```{r}
extract_spline_info <- function(model) {
  if (is.null(model$survey.design$variables)) {
    stop("Model must retain survey.design$variables for this to work.")
  }

  spline_info <- list()
  formula_terms <- all.vars(formula(model))

  # Extract terms that look like rcs(var, k)
  rcs_terms <- grep("rcs\\(", deparse(model$call$formula), value = TRUE)
  rcs_vars <- unlist(regmatches(rcs_terms, gregexpr("rcs\\([^\\)]+\\)", rcs_terms)))
  
  # Parse each rcs() call
  for (rcs_call in rcs_vars) {
    pieces <- gsub("rcs\\(([^,]+),\\s*([0-9]+)\\)", "\\1;\\2", rcs_call)
    split <- unlist(strsplit(pieces, ";"))
    varname <- trimws(split[1])
    nk <- as.numeric(split[2])

    # Get knot locations
    if (varname %in% names(model$survey.design$variables)) {
      var_data <- model$survey.design$variables[[varname]]
      spline_info[[varname]] <- Hmisc::rcspline.eval(var_data, nk = nk, knots.only = TRUE)
    }
  }

  return(spline_info)
}
```

### Factor variables

Preserve factor levels and contrasts:

```{r}
extract_factor_info <- function(model, glm_model = NULL) {
  factor_info <- list()
  
  # Get factor levels from xlevels
  if (length(model$xlevels) > 0) {
    factor_info$xlevels <- model$xlevels
  } else if (!is.null(glm_model) && length(glm_model$xlevels) > 0) {
    factor_info$xlevels <- glm_model$xlevels
  }
  
  # Get contrast information
  if (length(model$contrasts) > 0) {
    factor_info$contrasts <- model$contrasts
  } else if (!is.null(glm_model) && length(glm_model$contrasts) > 0) {
    factor_info$contrasts <- glm_model$contrasts
  }
  
  return(factor_info)
}
```

## Step 4: Creating a complete export function

Here's a comprehensive function that combines all the steps:

```{r}
create_safe_export <- function(svyglm_model, data_summary = NULL, glm_model = NULL) {
  
  # First, strip the model
  clean_model <- strip_svyglm(svyglm_model)
  
  # Extract all necessary components
  safe_export <- list(
    # Core components for prediction
    coefficients = coef(svyglm_model),
    vcov = vcov(svyglm_model),
    
    # Model specification
    formula = formula(svyglm_model),
    family = svyglm_model$family,
    
    # Degrees of freedom and dispersion
    df.residual = svyglm_model$df.residual,
    dispersion = summary(svyglm_model)$dispersion,
    
    # Survey design summary (no individual data)
    survey_info = clean_survey_design(svyglm_model$survey.design),
    
    # Transformation information
    centering = if (!is.null(data_summary)) {
      extract_centering_info(svyglm_model, data_summary)
    } else NULL,
    
    spline_info = extract_spline_info(svyglm_model),
    factor_info = extract_factor_info(svyglm_model, glm_model),
    
    # For recalibration
    original_prevalence = if (svyglm_model$family$family == "quasibinomial") {
      # This should be calculated in secure environment
      data_summary$outcome_prevalence
    } else NULL
  )
  
  class(safe_export) <- "safe_svyglm"
  return(safe_export)
}
```

## Step 5: Making predictions with the exported model

Create a predict method for the safe export:

```{r}
predict.safe_svyglm <- function(object, newdata, type = "response", se.fit = FALSE) {

  # Create formula with no outcome (right-hand side only)
  rhs_formula <- reformulate(attr(terms(object$formula), "term.labels"))

  # Build model matrix
  X <- model.matrix(rhs_formula, newdata)

  # Linear predictor
  eta <- as.vector(X %*% object$coefficients)

  # Output depending on link/response
  if (type == "link") {
    return(eta)
  } else if (type == "response") {
    return(object$family$linkinv(eta))  # e.g. logistic transform
  } else {
    stop("Unsupported type")
  }
}
```

## Step 6: Model recalibration

### Simple intercept recalibration

For models with centered predictors, adjusting for different population prevalence:

```{r}
recalibrate_intercept <- function(safe_model, new_prevalence) {
  if (is.null(safe_model$original_prevalence)) {
    stop("Original prevalence not available in exported model")
  }
  
  # Calculate adjustment on log-odds scale
  original_logit <- qlogis(safe_model$original_prevalence)
  new_logit <- qlogis(new_prevalence)
  adjustment <- new_logit - original_logit
  
  # Create recalibrated model
  recalibrated <- safe_model
  recalibrated$coefficients["(Intercept)"] <- 
    recalibrated$coefficients["(Intercept)"] + adjustment
  
  return(recalibrated)
}
```

### Full recalibration with new data

When you have labeled data in the new environment:

```{r}
recalibrate_model <- function(safe_model, new_data, new_outcomes, method = "logistic") {
  
  if (method == "logistic") {
    # Logistic recalibration: adjust intercept and slope
    original_pred <- predict(safe_model, new_data, type = "link")
    
    recal_fit <- glm(new_outcomes ~ original_pred, family = binomial())
    
    # Apply recalibration
    recalibrated <- safe_model
    recalibrated$coefficients <- recalibrated$coefficients * coef(recal_fit)[2]
    recalibrated$coefficients["(Intercept)"] <- 
      recalibrated$coefficients["(Intercept)"] + coef(recal_fit)[1]
    
    return(recalibrated)
  }
  
  if (method == "platt") {
    # Platt scaling - similar but preserves more of original model
    # Implementation here...
  }
}
```

### Bayesian recalibration

Using the survey model as a prior:

```{r}
prepare_for_bayesian_recal <- function(safe_model) {
  # Extract components needed for Bayesian updating
  prior_info <- list(
    beta_mean = safe_model$coefficients,
    beta_precision = solve(safe_model$vcov),  # Inverse of covariance
    df = safe_model$df.residual
  )
  
  # For use with Stan or other Bayesian frameworks
  return(prior_info)
}
```

## Step 7: Validation and quality checks

### Before export (in secure environment):

```{r}
validate_export <- function(original_model, safe_export, new_data) {
  # 1. Check no individual data remains
  individual_data_check <- function(obj) {
    dangerous_names <- c("residuals", "fitted.values", "y", "model", 
                        "weights", "data", "linear.predictors")
    
    for (name in dangerous_names) {
      if (!is.null(obj[[name]]) && length(obj[[name]]) > 0) {
        return(FALSE)
      }
    }
    return(TRUE)
  }
  
  # 2. Check predictions match
  orig_pred <- predict(original_model, newdata = new_data, type = "response")
  safe_pred <- predict.safe_svyglm(safe_export, newdata = new_data, type = "response")
  
  prediction_match <- all.equal(as.numeric(orig_pred), as.numeric(safe_pred), tolerance = 1e-10)
  
  # 3. Check components preserved
  components_ok <- all(
    identical(coef(original_model), safe_export$coefficients),
    all.equal(vcov(original_model), safe_export$vcov),
    identical(formula(original_model), safe_export$formula)
  )
  
  return(list(
    no_individual_data = individual_data_check(safe_export),
    predictions_match = prediction_match,
    components_preserved = components_ok
  ))
}
```

### After import (in new environment):

```{r}
assess_calibration <- function(safe_model, new_data, new_outcomes) {
  predictions <- predict(safe_model, new_data, type = "response")
  
  # Calibration plot data
  calibration_df <- data.frame(
    pred = predictions,
    obs = new_outcomes
  ) %>%
    mutate(bin = cut(pred, breaks = seq(0, 1, 0.1))) %>%
    group_by(bin) %>%
    summarise(
      mean_pred = mean(pred),
      mean_obs = mean(obs),
      n = n()
    )
  
  # Hosmer-Lemeshow test
  hl_test <- ResourceSelection::hoslem.test(new_outcomes, predictions)
  
  # Calibration slope and intercept
  calib_model <- glm(new_outcomes ~ qlogis(predictions), family = binomial())
  
  return(list(
    calibration_data = calibration_df,
    hosmer_lemeshow = hl_test,
    calibration_intercept = coef(calib_model)[1],
    calibration_slope = coef(calib_model)[2]
  ))
}
```

## Complete workflow example

Here's how to use all these functions together.

In secure environment:

```{r}
# 1. Fit your models
weighted_male_c$degf <- 68
male_model <- survey::svyglm(highbp14090_adj ~ clc_age_rcs_1_C + clc_age_rcs_2_C + clc_age_rcs_3_C + married_2_C + married_3_C + edudr04_1_C + edudr04_2_C + working_2_C + gendmhi_1_C + gen_025_2_C + gen_045_2_C + fmh_15_1_C + hwmdbmi_rcs_1_C + hwmdbmi_rcs_2_C + low_drink_score1_2_C + low_drink_score1_3_C + low_drink_score1_4_C + minperweek_rcs_1_C + minperweek_rcs_2_C + smoke_1_C + smoke_2_C + slp_11_C + totalfv_C + diabx_1_C + ckd_1_C + clc_age_rcs_1_by_gen_045_2_C + clc_age_rcs_2_by_gen_045_2_C + clc_age_rcs_3_by_gen_045_2_C + clc_age_rcs_1_by_hwmdbmi_rcs_1_C + clc_age_rcs_2_by_hwmdbmi_rcs_1_C + clc_age_rcs_3_by_hwmdbmi_rcs_1_C +  clc_age_rcs_1_by_hwmdbmi_rcs_2_C + clc_age_rcs_2_by_hwmdbmi_rcs_2_C + clc_age_rcs_3_by_hwmdbmi_rcs_2_C + clc_age_rcs_1_by_minperweek_rcs_1_C + 
clc_age_rcs_2_by_minperweek_rcs_1_C + clc_age_rcs_3_by_minperweek_rcs_1_C + 
clc_age_rcs_1_by_minperweek_rcs_2_C + clc_age_rcs_2_by_minperweek_rcs_2_C + 
clc_age_rcs_3_by_minperweek_rcs_2_C + clc_age_rcs_1_by_smoke_1_C + 
clc_age_rcs_2_by_smoke_1_C + clc_age_rcs_3_by_smoke_1_C + clc_age_rcs_1_by_smoke_2_C + clc_age_rcs_2_by_smoke_2_C + clc_age_rcs_3_by_smoke_2_C + 
clc_age_rcs_1_by_slp_11_C + clc_age_rcs_2_by_slp_11_C + clc_age_rcs_3_by_slp_11_C +
clc_age_rcs_1_by_diabx_1_C + clc_age_rcs_2_by_diabx_1_C + 
clc_age_rcs_3_by_diabx_1_C + clc_age_rcs_1_by_ckd_1_C + clc_age_rcs_2_by_ckd_1_C + clc_age_rcs_3_by_ckd_1_C, design = weighted_male_c, family = quasibinomial())

weighted_female_c$degf <- 68
female_model <- survey::svyglm(highbp14090_adj ~ clc_age_rcs_1_C + clc_age_rcs_2_C + clc_age_rcs_3_C + married_2_C + married_3_C + edudr04_1_C + edudr04_2_C + working_2_C + gendmhi_1_C + gen_025_2_C + gen_045_2_C + fmh_15_1_C + hwmdbmi_rcs_1_C + hwmdbmi_rcs_2_C + low_drink_score1_2_C + low_drink_score1_3_C + low_drink_score1_4_C + minperweek_rcs_1_C + minperweek_rcs_2_C + smoke_1_C + smoke_2_C + slp_11_C + totalfv_C + diabx_1_C + ckd_1_C + clc_age_rcs_1_by_gen_045_2_C + clc_age_rcs_2_by_gen_045_2_C + clc_age_rcs_3_by_gen_045_2_C + clc_age_rcs_1_by_hwmdbmi_rcs_1_C + clc_age_rcs_2_by_hwmdbmi_rcs_1_C + clc_age_rcs_3_by_hwmdbmi_rcs_1_C +  clc_age_rcs_1_by_hwmdbmi_rcs_2_C + clc_age_rcs_2_by_hwmdbmi_rcs_2_C + clc_age_rcs_3_by_hwmdbmi_rcs_2_C + clc_age_rcs_1_by_minperweek_rcs_1_C +
clc_age_rcs_2_by_minperweek_rcs_1_C + clc_age_rcs_3_by_minperweek_rcs_1_C +
clc_age_rcs_1_by_minperweek_rcs_2_C + clc_age_rcs_2_by_minperweek_rcs_2_C +
clc_age_rcs_3_by_minperweek_rcs_2_C + clc_age_rcs_1_by_smoke_1_C +
clc_age_rcs_2_by_smoke_1_C + clc_age_rcs_3_by_smoke_1_C + clc_age_rcs_1_by_smoke_2_C + clc_age_rcs_2_by_smoke_2_C + clc_age_rcs_3_by_smoke_2_C +
clc_age_rcs_1_by_slp_11_C + clc_age_rcs_2_by_slp_11_C + clc_age_rcs_3_by_slp_11_C +
clc_age_rcs_1_by_diabx_1_C + clc_age_rcs_2_by_diabx_1_C +
clc_age_rcs_3_by_diabx_1_C + clc_age_rcs_1_by_ckd_1_C + clc_age_rcs_2_by_ckd_1_C + clc_age_rcs_3_by_ckd_1_C, design = weighted_female_c, family = quasibinomial())

weighted_male_c$degf <- 68
male_reduced_model <- survey::svyglm(highbp14090_adj ~ clc_age_rcs_1_C + clc_age_rcs_2_C + clc_age_rcs_3_C + fmh_15_1_C + hwmdbmi_rcs_1_C + hwmdbmi_rcs_2_C + diabx_1_C + clc_age_rcs_1_by_hwmdbmi_rcs_1_C + clc_age_rcs_2_by_hwmdbmi_rcs_1_C + clc_age_rcs_3_by_hwmdbmi_rcs_1_C +  clc_age_rcs_1_by_hwmdbmi_rcs_2_C + clc_age_rcs_2_by_hwmdbmi_rcs_2_C + clc_age_rcs_3_by_hwmdbmi_rcs_2_C + clc_age_rcs_1_by_diabx_1_C + clc_age_rcs_2_by_diabx_1_C + 
clc_age_rcs_3_by_diabx_1_C, design = weighted_male_c, family = quasibinomial())

weighted_female_c$degf <- 68
female_reduced_model <- survey::svyglm(highbp14090_adj ~ clc_age_rcs_1_C + clc_age_rcs_2_C + clc_age_rcs_3_C + fmh_15_1_C + hwmdbmi_rcs_1_C + hwmdbmi_rcs_2_C + diabx_1_C + clc_age_rcs_1_by_hwmdbmi_rcs_1_C + clc_age_rcs_2_by_hwmdbmi_rcs_1_C + clc_age_rcs_3_by_hwmdbmi_rcs_1_C +  clc_age_rcs_1_by_hwmdbmi_rcs_2_C + clc_age_rcs_2_by_hwmdbmi_rcs_2_C + clc_age_rcs_3_by_hwmdbmi_rcs_2_C + clc_age_rcs_1_by_diabx_1_C + clc_age_rcs_2_by_diabx_1_C +
clc_age_rcs_3_by_diabx_1_C, design = weighted_female_c, family = quasibinomial())

# 2. Create data summary (without individual values)
male_data_summary <- list(
  clc_age = list(mean = 46.1451),
  married_2 = list(mean = 0.0859),
  married_3 = list(mean = 0.2485),
  edudr04_1 = list(mean = 0.2172),
  edudr04_2 = list(mean = 0.1251),
  working = list(mean = 0.2587),
  gendmhi = list(mean = 0.0668),
  gen_025 = list(mean = 0.2284),
  gen_045 = list(mean = 0.3499),
  fmh_15 = list(mean = 0.5248),
  hwmdbmi = list(mean = 27.66777),
  low_drink_score1_2 = list(mean = 0.778),
  low_drink_score1_3 = list(mean = 0.0456),
  low_drink_score1_4 = list(mean = 0.0722),
  minperweek = list(mean = 147.6497),
  smoke_1 = list(mean = 0.3087),
  smoke_2 = list(mean = 0.2277),
  slp_11 = list(mean = 7.015036),
  totalfv = list(mean = 3.280092),
  diabx = list(mean = 0.1049),
  ckd = list(mean = 0.0448),
  outcome_prevalence = 0.25
)

female_data_summary <- list(
  clc_age = list(mean = 46.89546),
  married_2 = list(mean = 0.1471),
  married_3 = list(mean = 0.2101),
  edudr04_1 = list(mean = 0.1934),
  edudr04_2 = list(mean = 0.0997),
  working = list(mean = 0.3534),
  gendmhi = list(mean = 0.0885),
  gen_025 = list(mean = 0.258),
  gen_045 = list(mean = 0.3427),
  fmh_15 = list(mean = 0.5736),
  hwmdbmi = list(mean = 27.09638),
  low_drink_score1_2 = list(mean = 0.7532),
  low_drink_score1_3 = list(mean = 0.0385),
  low_drink_score1_4 = list(mean = 0.032),
  minperweek = list(mean = 126.2692),
  smoke_1 = list(mean = 0.2722),
  smoke_2 = list(mean = 0.1652),
  slp_11 = list(mean = 7.176309),
  totalfv = list(mean = 3.70042),
  diabx = list(mean = 0.0734),
  ckd = list(mean = 0.0576),
  outcome_prevalence = 0.22
)

# 3. Create safe export
male_safe_model <- create_safe_export(male_model, male_data_summary)
female_safe_model <- create_safe_export(female_model, female_data_summary)
male_safe_reduced_model <- create_safe_export(male_reduced_model, male_data_summary)
female_safe_reduced_model <- create_safe_export(female_reduced_model, female_data_summary)

# 4. Validate with opposite data before saving
validate_export(male_model, male_safe_model, female_data_c)
validate_export(female_model, female_safe_model, male_data_c)
validate_export(male_reduced_model, male_safe_reduced_model, female_data_c)
validate_export(female_reduced_model, female_safe_reduced_model, male_data_c)
 
# 5. Remove large formula components and save objects
male_safe_model$formula <- NULL
female_safe_model$formula <- NULL
male_safe_reduced_model$formula <- NULL
female_safe_reduced_model$formula <- NULL

saveRDS(male_safe_model, file = "P:/10619/Dropbox/htnport/models/svyglm/male_model.rds")
saveRDS(female_safe_model, file = "P:/10619/Dropbox/htnport/models/svyglm/female_model.rds")
saveRDS(male_safe_reduced_model, file = "P:/10619/Dropbox/htnport/models/svyglm/male_reduced_model.rds")
saveRDS(female_safe_reduced_model, file = "P:/10619/Dropbox/htnport/models/svyglm/female_reduced_model.rds")
```

In new environment:

```{r}
# 6. Load, restore formulas, and use
male_safe_reduced_model <- readRDS("P:/10619/Dropbox/htnport/models/svyglm/male_reduced_model.rds")
female_safe_reduced_model <- readRDS("P:/10619/Dropbox/htnport/models/svyglm/female_reduced_model.rds")

male_safe_reduced_model$formula <- highbp14090_adj ~ clc_age_rcs_1_C + clc_age_rcs_2_C + clc_age_rcs_3_C + fmh_15_1_C + hwmdbmi_rcs_1_C + hwmdbmi_rcs_2_C + diabx_1_C + clc_age_rcs_1_by_hwmdbmi_rcs_1_C + clc_age_rcs_2_by_hwmdbmi_rcs_1_C + clc_age_rcs_3_by_hwmdbmi_rcs_1_C + clc_age_rcs_1_by_hwmdbmi_rcs_2_C + clc_age_rcs_2_by_hwmdbmi_rcs_2_C + clc_age_rcs_3_by_hwmdbmi_rcs_2_C + clc_age_rcs_1_by_diabx_1_C + clc_age_rcs_2_by_diabx_1_C + clc_age_rcs_3_by_diabx_1_C
female_safe_reduced_model$formula <- highbp14090_adj ~ clc_age_rcs_1_C + clc_age_rcs_2_C + clc_age_rcs_3_C + fmh_15_1_C + hwmdbmi_rcs_1_C + hwmdbmi_rcs_2_C + diabx_1_C + clc_age_rcs_1_by_hwmdbmi_rcs_1_C + clc_age_rcs_2_by_hwmdbmi_rcs_1_C + clc_age_rcs_3_by_hwmdbmi_rcs_1_C + clc_age_rcs_1_by_hwmdbmi_rcs_2_C + clc_age_rcs_2_by_hwmdbmi_rcs_2_C + clc_age_rcs_3_by_hwmdbmi_rcs_2_C + clc_age_rcs_1_by_diabx_1_C + clc_age_rcs_2_by_diabx_1_C + clc_age_rcs_3_by_diabx_1_C

# 7. Make predictions using whole unpacked centered data
new_male_predictions <- predict(male_safe_reduced_model, newdata = female_data_c, type = "response")
new_female_predictions <- predict(female_safe_reduced_model, newdata = male_data_c, type = "response")

# 8. Assess calibration if you have outcomes
if (!is.null(new_male_predictions)) {
  calibration <- assess_calibration(male_safe_reduced_model, new_data, new_outcomes)
  
  # 9. Recalibrate if needed
  if (calibration$calibration_slope < 0.8) {
    recalibrated_model <- recalibrate_model(male_safe_reduced_model, new_data, 
                                           new_male_predictions, method = "logistic")
  }
}

if (!is.null(new_female_predictions)) {
  calibration <- assess_calibration(female_safe_reduced_model, new_data, new_outcomes)
  
  # 9. Recalibrate if needed
  if (calibration$calibration_slope < 0.8) {
    recalibrated_model <- recalibrate_model(female_safe_reduced_model, new_data, 
                                           new_female_predictions, method = "logistic")
  }
}
```

## Summary of key considerations

1.  **Privacy first**: Never include individual-level data in exports. This includes residuals, fitted values, and the original data frame.

2.  **Preserve statistical components**: The variance-covariance matrix is critical for uncertainty quantification and recalibration. Always include it.

3.  **Document transformations**: Centering, scaling, and spline knots must be explicitly preserved as they're not always recoverable from the model object.

4.  **Plan for recalibration**: Include prevalence information and design metadata to support various recalibration approaches.

5.  **Validate thoroughly**: Check both that individual data is removed AND that predictions remain accurate.

6.  **Consider RCS complexity**: Spline terms require extra care - knot locations and any centering must be preserved together.

This approach provides a secure, validated method for model export that maintains predictive accuracy while protecting individual privacy.
