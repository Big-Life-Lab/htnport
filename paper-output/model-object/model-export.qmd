---
title: "Exporting HTNPoRT Models"
format: html
editor: visual
---

## Summary

This guide provides a step-by-step approach for safely exporting survey-weighted GLM models from Statistics Canada while preserving the ability to make predictions and perform recalibration in new settings. The key challenge is removing all individual-level and identifiable data while retaining the statistical components necessary for valid inference.

**Key Steps:**

1.  Strip all individual-level data from the model object
2.  Preserve essential statistical components (coefficients, variance-covariance matrix)
3.  Extract and save transformation parameters (centering, scaling, spline knots)
4.  Document survey design metadata without individual identifiers
5.  Validate the export before transfer
6.  Implement recalibration methods in the new environment

## Understanding a svyglm object

A `svyglm` object inherits from both the `svyglm` and `glm` classes, containing standard GLM components plus survey-specific information:

**Standard GLM components:**

-   `coefficients`: Model parameter estimates
-   `residuals`: Individual-level residuals (MUST BE REMOVED)
-   `fitted.values`: Individual predictions (MUST BE REMOVED)
-   `model`: Original data frame (MUST BE REMOVED)
-   `family`: Distribution and link function (SAFE TO KEEP)
-   `formula`: Model specification (SAFE TO KEEP)
-   `qr`: QR decomposition of design matrix (REMOVE - not needed for prediction)

**Survey-specific components:**

-   `survey.design`: Contains survey weights, strata, clusters, and THE ORIGINAL DATA (MUST BE REMOVED)
-   `vcov`: Variance-covariance matrix using sandwich estimators (CRITICAL TO KEEP)
-   `df.residual`: Degrees of freedom accounting for survey design (KEEP)

The embedded survey design object poses the primary privacy risk as it contains all individual-level data.

## Step 1: Basic model stripping

Start with a basic function to remove individual-level data:

```{r}
strip_svyglm <- function(model) {
  # Remove all individual-level components
  model$y <- NULL
  model$model <- NULL
  model$residuals <- NULL
  model$fitted.values <- NULL
  model$effects <- NULL
  model$qr <- NULL
  model$linear.predictors <- NULL
  model$weights <- NULL
  model$prior.weights <- NULL
  model$data <- NULL
  
  # Remove the environment to prevent data access
  attr(model$terms, ".Environment") <- emptyenv()
  attr(model$formula, ".Environment") <- emptyenv()
  
  return(model)
}
```

## Step 2: Handling survey design information

The survey design object requires special care. We need to preserve the structure but remove individual data:

```{r}
clean_survey_design <- function(design) {
  # Create a summary that preserves structure without data
  design_summary <- list(
    nobs = nrow(design$variables),
    nstrata = length(unique(design$strata)),
    nclusters = length(unique(design$cluster)),
    has_weights = !is.null(design$prob),
    design_type = class(design)[1],
    call = design$call
  )
  
  # For finite population correction
  if (!is.null(design$fpc)) {
    design_summary$has_fpc <- TRUE
    design_summary$fpc_summary <- "FPC information available"
  }
  
  return(design_summary)
}
```

## Step 3: Preserving complex model features

### Centering and Scaling

When predictors are centered and scaled, you must preserve these transformations:

```{r}
extract_centering_info <- function(model, original_data_summary) {
  centering_info <- list()
  
  # Extract predictor names from model formula (excluding response)
  predictors <- all.vars(formula(model))[-1]
  
  for (var in predictors) {
    # Match all summary keys that start with var
    matching_keys <- grep(paste0("^", var), names(original_data_summary), value = TRUE)
    
    if (length(matching_keys) > 1) {
      centering_info[[var]] <- list()
      for (i in seq_along(matching_keys)) {
        mean_val <- original_data_summary[[matching_keys[i]]]$mean
        centering_info[[var]][[paste0("center", i)]] <- mean_val
      }
    } else if (var %in% names(original_data_summary)) {
      centering_info[[var]] <- list(center = original_data_summary[[var]]$mean)
    }
  }
  
  return(centering_info)
}
```

### Restricted cubic splines (RCS)

RCS terms require special handling because knot locations must be preserved:

```{r}
extract_spline_info <- function(model) {
  if (is.null(model$survey.design$variables)) {
    stop("Model must retain survey.design$variables for this to work.")
  }

  spline_info <- list()
  formula_terms <- all.vars(formula(model))

  # Extract terms that look like rcs(var, k)
  rcs_terms <- grep("rcs\\(", deparse(model$call$formula), value = TRUE)
  rcs_vars <- unlist(regmatches(rcs_terms, gregexpr("rcs\\([^\\)]+\\)", rcs_terms)))
  
  # Parse each rcs() call
  for (rcs_call in rcs_vars) {
    pieces <- gsub("rcs\\(([^,]+),\\s*([0-9]+)\\)", "\\1;\\2", rcs_call)
    split <- unlist(strsplit(pieces, ";"))
    varname <- trimws(split[1])
    nk <- as.numeric(split[2])

    # Get knot locations
    if (varname %in% names(model$survey.design$variables)) {
      var_data <- model$survey.design$variables[[varname]]
      spline_info[[varname]] <- Hmisc::rcspline.eval(var_data, nk = nk, knots.only = TRUE)
    }
  }

  return(spline_info)
}
```

### Factor variables

Preserve factor levels and contrasts:

```{r}
extract_factor_info <- function(model, glm_model = NULL) {
  factor_info <- list()
  
  # Get factor levels from xlevels
  if (length(model$xlevels) > 0) {
    factor_info$xlevels <- model$xlevels
  } else if (!is.null(glm_model) && length(glm_model$xlevels) > 0) {
    factor_info$xlevels <- glm_model$xlevels
  }
  
  # Get contrast information
  if (length(model$contrasts) > 0) {
    factor_info$contrasts <- model$contrasts
  } else if (!is.null(glm_model) && length(glm_model$contrasts) > 0) {
    factor_info$contrasts <- glm_model$contrasts
  }
  
  return(factor_info)
}
```

## Step 4: Creating a complete export function

Here's a comprehensive function that combines all the steps:

```{r}
create_safe_export <- function(svyglm_model, data_summary = NULL, glm_model = NULL) {
  
  # First, strip the model
  clean_model <- strip_svyglm(svyglm_model)
  
  # Extract all necessary components
  safe_export <- list(
    # Core components for prediction
    coefficients = coef(svyglm_model),
    vcov = vcov(svyglm_model),
    
    # Model specification
    formula = formula(svyglm_model),
    family = svyglm_model$family,
    
    # Degrees of freedom and dispersion
    df.residual = svyglm_model$df.residual,
    dispersion = summary(svyglm_model)$dispersion,
    
    # Survey design summary (no individual data)
    survey_info = clean_survey_design(svyglm_model$survey.design),
    
    # Transformation information
    centering = if (!is.null(data_summary)) {
      extract_centering_info(svyglm_model, data_summary)
    } else NULL,
    
    spline_info = extract_spline_info(svyglm_model),
    factor_info = extract_factor_info(svyglm_model, glm_model),
    
    # For recalibration
    original_prevalence = if (svyglm_model$family$family == "quasibinomial") {
      # This should be calculated in secure environment
      data_summary$outcome_prevalence
    } else NULL
  )
  
  class(safe_export) <- "safe_svyglm"
  return(safe_export)
}
```

## Step 5: Making predictions with the exported model

Create a predict method for the safe export:

```{r}
predict.safe_svyglm <- function(object, newdata, type = "response", se.fit = FALSE) {

  # Create formula with no outcome (right-hand side only)
  rhs_formula <- reformulate(attr(terms(object$formula), "term.labels"))

  # Build model matrix
  X <- model.matrix(rhs_formula, newdata)

  # Linear predictor
  eta <- as.vector(X %*% object$coefficients)

  # Output depending on link/response
  if (type == "link") {
    return(eta)
  } else if (type == "response") {
    return(object$family$linkinv(eta))  # e.g. logistic transform
  } else {
    stop("Unsupported type")
  }
}
```

## Step 6: Validation and quality checks

```{r}
validate_export <- function(obj) {
    dangerous_names <- c("residuals", "fitted.values", "y", "model", 
                        "weights", "data", "linear.predictors")
    
    for (name in dangerous_names) {
      if (!is.null(obj[[name]]) && length(obj[[name]]) > 0) {
        return(FALSE)
      }
    }
    return(TRUE)
}
```

## Complete workflow example

Here's how to use all these functions together.

In secure environment:

```{r}
# 1. Fit your models
weighted_male_c$degf <- 68
male_final_model <- survey::svyglm(highbp14090_adj ~ clc_age_rcs_1_C + clc_age_rcs_2_C + clc_age_rcs_3_C + fmh_15_1_C + hwmdbmi_rcs_1_C + hwmdbmi_rcs_2_C + diabx_1_C + clc_age_rcs_1_by_hwmdbmi_rcs_1_C + clc_age_rcs_2_by_hwmdbmi_rcs_1_C + clc_age_rcs_3_by_hwmdbmi_rcs_1_C +  clc_age_rcs_1_by_hwmdbmi_rcs_2_C + clc_age_rcs_2_by_hwmdbmi_rcs_2_C + clc_age_rcs_3_by_hwmdbmi_rcs_2_C + clc_age_rcs_1_by_diabx_1_C + clc_age_rcs_2_by_diabx_1_C + 
clc_age_rcs_3_by_diabx_1_C, design = weighted_male_c, family = quasibinomial())

weighted_female_c$degf <- 68
female_final_model <- survey::svyglm(highbp14090_adj ~ clc_age_rcs_1_C + clc_age_rcs_2_C + clc_age_rcs_3_C + fmh_15_1_C + hwmdbmi_rcs_1_C + hwmdbmi_rcs_2_C + diabx_1_C + clc_age_rcs_1_by_hwmdbmi_rcs_1_C + clc_age_rcs_2_by_hwmdbmi_rcs_1_C + clc_age_rcs_3_by_hwmdbmi_rcs_1_C +  clc_age_rcs_1_by_hwmdbmi_rcs_2_C + clc_age_rcs_2_by_hwmdbmi_rcs_2_C + clc_age_rcs_3_by_hwmdbmi_rcs_2_C + clc_age_rcs_1_by_diabx_1_C + clc_age_rcs_2_by_diabx_1_C +
clc_age_rcs_3_by_diabx_1_C, design = weighted_female_c, family = quasibinomial())

# 2. Create data summary (without individual values)
male_data_summary <- list(
  clc_age = list(mean = 46.1451),
  fmh_15 = list(mean = 0.5248),
  hwmdbmi = list(mean = 27.66777),
  outcome_prevalence = 0.25
)

female_data_summary <- list(
  clc_age = list(mean = 46.89546),
  fmh_15 = list(mean = 0.5736),
  hwmdbmi = list(mean = 27.09638),
  diabx = list(mean = 0.0734),
  outcome_prevalence = 0.22
)

# 3. Create safe export
male_safe_final_model <- create_safe_export(male_final_model, male_data_summary)
female_safe_final_model <- create_safe_export(female_final_model, female_data_summary)

# 4. Save model objects
saveRDS(male_safe_final_model, file = here::here("output", "model-object", "male_model.rds"))
savedRDS(female_safe_final_model, file = here::here("output", "model-object", "female_model.rds"))
```

In new environment:

```{r}
# 5. Load model objects
male_safe_final_model <- readRDS(here::here("output", "model-object", "male_model.rds"))
female_safe_final_model <- readRDS(here::here("output", "model-object", "female_model.rds"))

# 6. Validate cleanliness of model objects
testthat::test_that("Model objects are clean", {
  testthat::expect_true(validate_export(male_safe_final_model))
  testthat::expect_true(validate_export(female_safe_final_model))
})

# 7. Load validation data
male_validation_data <- read.csv(here::here("output", "validation-data", "HTNPoRT-male-validation-data.csv"))
female_validation_data  <- read.csv(here::here("output", "validation-data", "HTNPoRT-female-validation-data.csv"))

# 8. Make predictions using validation data
new_male_predictions <- predict(male_safe_final_model, newdata = male_validation_data , type = "response")
new_female_predictions <- predict(female_safe_final_model, newdata = female_validation_data, type = "response")
```

## Summary of key considerations

1.  **Privacy first**: Never include individual-level data in exports. This includes residuals, fitted values, and the original data frame.

2.  **Preserve statistical components**: The variance-covariance matrix is critical for uncertainty quantification and recalibration. Always include it.

3.  **Document transformations**: Centering, scaling, and spline knots must be explicitly preserved as they're not always recoverable from the model object.

4.  **Plan for recalibration**: Include prevalence information and design metadata to support various recalibration approaches.

5.  **Validate thoroughly**: Check both that individual data is removed AND that predictions remain accurate.

6.  **Consider RCS complexity**: Spline terms require extra care - knot locations and any centering must be preserved together.

This approach provides a secure, validated method for model export that maintains predictive accuracy while protecting individual privacy.
